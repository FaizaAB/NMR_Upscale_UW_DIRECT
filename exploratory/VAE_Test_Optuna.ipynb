{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe13cb26-f94d-4a4c-8ea7-84837b308736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Loading in libraries necessary for CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, BatchNorm1d, ConvTranspose1d\n",
    "#from torch.optim import Adam, SGD, Adagrad, RMSprop, SparseAdam, LBFGS, Adadelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import nmrsim\n",
    "from nmrsim import plt\n",
    "from itertools import product\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import optuna \n",
    "\n",
    "# whether to run on GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "#print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283f4459-bef4-4913-b11f-f132fb9b56ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  1294\n"
     ]
    }
   ],
   "source": [
    "#Checking how many files are in repository for training, testing, and validation\n",
    "files = glob.glob('./Spectral_Data/spectral_data/400MHz/*.csv')\n",
    "print('Total number of files: ', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57d8e56-fe6a-42b2-b6f1-d40e1f01846c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GHzData(Dataset):\n",
    "    def __init__(self):\n",
    "        # Data loading starting with list of csv strings\n",
    "        self.files = glob.glob(os.path.join('./Spectral_Data/spectral_data/400MHz', \n",
    "                                                       'spectral_data_*.csv'))\n",
    "\n",
    "        self.y_60 = [] # Establishes a list for 60 MHz data\n",
    "        self.y_400 = [] # Establishes a list for 400 MHz data\n",
    "\n",
    "        for self.file in self.files: # For loop for each file in files\n",
    "            self.df = pd.read_csv(self.file) # Reads each into a pandas dataframe\n",
    "            self.array_60 = self.df['60MHz_intensity'].to_numpy() # Takes 60MHz intensity to np\n",
    "            self.array_400 = self.df['400MHz_intensity'].to_numpy() # Takes 400MHz intensity to np\n",
    "            self.y_60.append(self.array_60) # Appends all arrays to 60MHz list\n",
    "            self.y_400.append(self.array_400) # Appends all arrays to 400MHz list\n",
    "            \n",
    "        # Creates a 60 MHz tensor from list, converts to float, unsqueezes to have shape (n, 1, 5500)\n",
    "        self.tensor_60 = torch.Tensor(self.y_60).float().unsqueeze(1).to(device)        \n",
    "\n",
    "        # Creates a 400 MHz tensor from list, converts to float, unsqueezes to have shape (n, 1, 5500)\n",
    "        self.tensor_400 = torch.Tensor(self.y_400).float().unsqueeze(1).to(device)\n",
    "        \n",
    "        # Track the length of number of samples in frame\n",
    "        self.num_samples = len(self.y_60)\n",
    "\n",
    "    def __getitem__(self, index): # establishes an index for the tensors\n",
    "        return self.tensor_60[index], self.tensor_400[index]\n",
    "    \n",
    "    def __len__(self): # Returns variable number of samples\n",
    "        return self.num_samples\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462c7ebc-b9f9-443c-881f-fdb3c6ef5e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        data, _ = data\n",
    "        data = data.to(device)\n",
    "        #data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40873643-0803-47f6-a21a-48c6ea573504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            data, _ = data\n",
    "            data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e514fb-4b24-4b43-b1ba-1628f7a7b714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59bbad63-b609-48d6-9672-288e9d02123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 5500\n",
    "encode = 512\n",
    "# define a simple linear VAE for Trial Optimization\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self,trial):\n",
    "        super(LinearVAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=5500, out_features=encode)\n",
    "        self.enc2 = nn.Linear(in_features=encode, out_features=features*2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=features, out_features=encode)\n",
    "        self.dec2 = nn.Linear(in_features=encode, out_features=5500)\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # add an additional dimension to the input tensor\n",
    "        x = x.view(-1, 5500)\n",
    "        x = x.unsqueeze(1)\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = self.enc2(x).view(-1, 2, features)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        #print(x.shape)\n",
    "        reconstruction = torch.sigmoid(self.dec2(x))\n",
    "        reconstruction = reconstruction.unsqueeze(1)\n",
    "        #print(reconstruction.shape)\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c97b2c-ff49-49eb-9c60-bd6671b72223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # Generate the model\n",
    "    model = LinearVAE(trial).to(device)\n",
    "    \n",
    "    # Generate optimizers\n",
    "    # Try Adam, AdaDelta, Adagrad, RMSprop, SGD\n",
    "    \n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'Adadelta', 'Adagrad', 'RMSprop', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    batch_size_trial = trial.suggest_int('batch_size', 64, 256, step=64)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 5, 50, step=5)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Load Data\n",
    "    # Establishing and loading data into notebook\n",
    "    dataset = GHzData()\n",
    "\n",
    "    #Splitting the data\n",
    "    train_X, test_X, train_y, test_y = train_test_split(dataset.tensor_60, dataset.tensor_400,\n",
    "                                                        test_size=0.1)\n",
    "\n",
    "    # Splits train data into validation data\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y,\n",
    "                                                          test_size=0.1)\n",
    "    # Creating datasets\n",
    "    train_dataset = TensorDataset(train_X, train_y)\n",
    "    test_dataset = TensorDataset(test_X, test_y)\n",
    "    valid_dataset = TensorDataset(valid_X, valid_y)\n",
    "\n",
    "    # Batch size change to higher batch sizes\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_trial, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size_trial, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_trial, shuffle=True)\n",
    "    \n",
    "    # Paste training loop here\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss = fit(model, train_dataloader, optimizer, criterion)\n",
    "        val_epoch_loss = validate(model, valid_dataloader, optimizer, criterion)\n",
    "\n",
    "    trial.report(train_epoch_loss, epoch)\n",
    "    \n",
    "    # Handle pruning\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "337dbe9b-55ab-4cc4-9afa-aaf8746d03db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 14:35:40,299]\u001b[0m A new study created in memory with name: no-name-b30c77d7-515a-470d-8161-3694ad6b1f64\u001b[0m\n",
      "/home/garrettreinhard/miniconda3/envs/isotope/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "\u001b[32m[I 2023-03-07 14:35:57,303]\u001b[0m Trial 0 finished with value: 1.0090171810321389 and parameters: {'optimizer': 'Adam', 'lr': 5.087058300521216e-05, 'batch_size': 256, 'num_epochs': 15}. Best is trial 0 with value: 1.0090171810321389.\u001b[0m\n",
      "\u001b[33m[W 2023-03-07 14:50:59,451]\u001b[0m Trial 1 failed with parameters: {'optimizer': 'RMSprop', 'lr': 0.07740451505572607, 'batch_size': 256, 'num_epochs': 40} because of the following error: The value nan is not acceptable..\u001b[0m\n",
      "\u001b[33m[W 2023-03-07 14:50:59,461]\u001b[0m Trial 1 failed with value nan.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:24:57,958]\u001b[0m Trial 2 finished with value: 0.01832569897914912 and parameters: {'optimizer': 'SGD', 'lr': 0.00011391151805942919, 'batch_size': 128, 'num_epochs': 35}. Best is trial 2 with value: 0.01832569897914912.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:09,024]\u001b[0m Trial 3 finished with value: 0.0005496193645050555 and parameters: {'optimizer': 'RMSprop', 'lr': 0.002876776781152226, 'batch_size': 256, 'num_epochs': 35}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:17,496]\u001b[0m Trial 4 finished with value: 0.09247176945494148 and parameters: {'optimizer': 'Adadelta', 'lr': 0.049921401512565286, 'batch_size': 256, 'num_epochs': 40}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:23,195]\u001b[0m Trial 5 finished with value: 0.0013227679525655912 and parameters: {'optimizer': 'SGD', 'lr': 0.002244387348749032, 'batch_size': 192, 'num_epochs': 10}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:28,693]\u001b[0m Trial 6 finished with value: 0.027021382121438627 and parameters: {'optimizer': 'RMSprop', 'lr': 0.003398408971830622, 'batch_size': 192, 'num_epochs': 5}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:35,987]\u001b[0m Trial 7 finished with value: 0.0013336373605154306 and parameters: {'optimizer': 'SGD', 'lr': 0.0018308997321392468, 'batch_size': 192, 'num_epochs': 45}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:41,899]\u001b[0m Trial 8 finished with value: 0.0038350663986679704 and parameters: {'optimizer': 'SGD', 'lr': 0.011615990544690756, 'batch_size': 64, 'num_epochs': 5}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n",
      "\u001b[32m[I 2023-03-07 15:25:47,998]\u001b[0m Trial 9 finished with value: 0.30143842496753537 and parameters: {'optimizer': 'Adagrad', 'lr': 0.00038961295917622245, 'batch_size': 256, 'num_epochs': 15}. Best is trial 3 with value: 0.0005496193645050555.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0005496193645050555\n",
      "Best hyperparameters: {'optimizer': 'RMSprop', 'lr': 0.002876776781152226, 'batch_size': 256, 'num_epochs': 35}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Training Loss: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8632a-d312-49bc-b1f2-0e64298fc0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b02ec7-b186-4a30-87ee-47de87a0c87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
