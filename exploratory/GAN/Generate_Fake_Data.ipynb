{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fba72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f4a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrumUpscaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrumUpscaler, self).__init__()\n",
    "        self.fc1 = nn.Linear(5500, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, 1024)\n",
    "        self.fc5 = nn.Linear(1024, 2048)\n",
    "        self.fc6 = nn.Linear(2048, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 5500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "        x = nn.functional.relu(self.fc5(x))\n",
    "        x = nn.functional.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b62d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_600=pd.DataFrame()\n",
    "df_600=pd.read_csv(\"600.csv\")\n",
    "\n",
    "df_60=pd.DataFrame()\n",
    "df_60=pd.read_csv(\"60.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9780c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# whether to run on GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "#print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4714d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Complete for epoch1\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch2\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch3\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch4\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch5\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch6\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch7\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch8\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch9\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch10\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch11\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch12\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch13\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch14\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch15\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch16\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch17\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch18\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch19\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch20\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch21\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch22\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch23\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch24\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch25\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch26\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch27\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch28\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch29\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch30\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch31\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch32\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch33\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch34\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch35\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch36\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch37\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch38\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch39\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch40\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch41\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch42\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch43\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch44\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch45\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch46\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch47\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch48\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch49\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch50\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch51\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch52\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch53\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch54\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch55\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch56\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch57\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch58\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch59\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch60\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch61\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch62\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch63\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch64\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch65\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch66\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch67\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch68\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch69\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch70\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch71\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch72\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch73\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch74\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch75\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch76\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch77\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch78\n",
      "Test Loss: 0.0005\n",
      "Traning Complete for epoch79\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch80\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch81\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch82\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch83\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch84\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch85\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch86\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch87\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch88\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch89\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch90\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch91\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch92\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch93\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch94\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch95\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch96\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch97\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch98\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch99\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch100\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch101\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch102\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch103\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch104\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch105\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch106\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch107\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch108\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch109\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch110\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch111\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch112\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch113\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch114\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch115\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch116\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch117\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch118\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch119\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch120\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch121\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch122\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch123\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch124\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch125\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch126\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch127\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch128\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch129\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch130\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch131\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch132\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch133\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch134\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch135\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch136\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch137\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch138\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch139\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch140\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch141\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch142\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch143\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch144\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch145\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch146\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch147\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch148\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch149\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch150\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch151\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch152\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch153\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch154\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch155\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch156\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch157\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch158\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch159\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch160\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch161\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch162\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch163\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch164\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch165\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch166\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch167\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch168\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch169\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch170\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch171\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch172\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch173\n",
      "Test Loss: 0.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Complete for epoch174\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch175\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch176\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch177\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch178\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch179\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch180\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch181\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch182\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch183\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch184\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch185\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch186\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch187\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch188\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch189\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch190\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch191\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch192\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch193\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch194\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch195\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch196\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch197\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch198\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch199\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch200\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch201\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch202\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch203\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch204\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch205\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch206\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch207\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch208\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch209\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch210\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch211\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch212\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch213\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch214\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch215\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch216\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch217\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch218\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch219\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch220\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch221\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch222\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch223\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch224\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch225\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch226\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch227\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch228\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch229\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch230\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch231\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch232\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch233\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch234\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch235\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch236\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch237\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch238\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch239\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch240\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch241\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch242\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch243\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch244\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch245\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch246\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch247\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch248\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch249\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch250\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch251\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch252\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch253\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch254\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch255\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch256\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch257\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch258\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch259\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch260\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch261\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch262\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch263\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch264\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch265\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch266\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch267\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch268\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch269\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch270\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch271\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch272\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch273\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch274\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch275\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch276\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch277\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch278\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch279\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch280\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch281\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch282\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch283\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch284\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch285\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch286\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch287\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch288\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch289\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch290\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch291\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch292\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch293\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch294\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch295\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch296\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch297\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch298\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch299\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch300\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch301\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch302\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch303\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch304\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch305\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch306\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch307\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch308\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch309\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch310\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch311\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch312\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch313\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch314\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch315\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch316\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch317\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch318\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch319\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch320\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch321\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch322\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch323\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch324\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch325\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch326\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch327\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch328\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch329\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch330\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch331\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch332\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch333\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch334\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch335\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch336\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch337\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch338\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch339\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch340\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch341\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch342\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch343\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch344\n",
      "Test Loss: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Complete for epoch345\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch346\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch347\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch348\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch349\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch350\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch351\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch352\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch353\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch354\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch355\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch356\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch357\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch358\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch359\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch360\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch361\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch362\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch363\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch364\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch365\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch366\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch367\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch368\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch369\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch370\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch371\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch372\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch373\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch374\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch375\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch376\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch377\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch378\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch379\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch380\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch381\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch382\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch383\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch384\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch385\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch386\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch387\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch388\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch389\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch390\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch391\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch392\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch393\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch394\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch395\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch396\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch397\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch398\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch399\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch400\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch401\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch402\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch403\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch404\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch405\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch406\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch407\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch408\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch409\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch410\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch411\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch412\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch413\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch414\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch415\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch416\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch417\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch418\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch419\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch420\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch421\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch422\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch423\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch424\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch425\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch426\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch427\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch428\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch429\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch430\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch431\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch432\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch433\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch434\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch435\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch436\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch437\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch438\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch439\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch440\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch441\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch442\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch443\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch444\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch445\n",
      "Test Loss: 0.0006\n",
      "Traning Complete for epoch446\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch447\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch448\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch449\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch450\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch451\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch452\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch453\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch454\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch455\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch456\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch457\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch458\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch459\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch460\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch461\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch462\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch463\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch464\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch465\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch466\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch467\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch468\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch469\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch470\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch471\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch472\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch473\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch474\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch475\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch476\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch477\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch478\n",
      "Test Loss: 0.0009\n",
      "Traning Complete for epoch479\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch480\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch481\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch482\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch483\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch484\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch485\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch486\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch487\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch488\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch489\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch490\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch491\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch492\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch493\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch494\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch495\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch496\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch497\n",
      "Test Loss: 0.0008\n",
      "Traning Complete for epoch498\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch499\n",
      "Test Loss: 0.0007\n",
      "Traning Complete for epoch500\n",
      "Test Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#generate fake data\n",
    "\n",
    "for j in range(1,501):\n",
    "    # Define the model\n",
    "    model = SpectrumUpscaler()\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    X_train, X_test,Y_train,Y_test = train_test_split(df_60,df_600,test_size=0.2, random_state=j)\n",
    "    X_test = torch.tensor(X_test.values).float()\n",
    "    Y_train = torch.tensor(Y_train.values).float()\n",
    "    Y_test = torch.tensor(Y_test.values).float()\n",
    "    X_train = torch.tensor(X_train.values).float()\n",
    "    \n",
    "    # Train the model\n",
    "    num_epochs = j\n",
    "    batch_size = j\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            # Get the batch\n",
    "            batch_x = X_train[i:i+batch_size]\n",
    "            batch_y = Y_train[i:i+batch_size]\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"Traning Complete for epoch{j}\")\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, Y_test)\n",
    "        print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "    df_output = pd.DataFrame(test_outputs.detach().numpy())\n",
    "    df_output.to_csv(f\"fake_data/fake_data_{j}\",index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ea36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bade0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
